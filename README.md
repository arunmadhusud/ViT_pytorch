# ViT_pytorch

This is an unofficial PyTorch implementation of Vision Transformer (ViT) introduced in the paper [An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale](https://arxiv.org/abs/2010.11929).

The goal of this project is to provide a simple and well_commented implementation of the Vision Transformer model in PyTorch. The weights of the model are loaded with the pretrained weights from the official implementation of the paper available in timm library.

## Acknowledgement:

Most of the code is referred from the Vision Transformer in PyTorch tutorial by jankrepl. The tutorial can be found [here](https://youtu.be/ovB0ddFtzzA?feature=shared)

The code for visualizing the attention maps is referred from the Pytorch reimplementation of Vision Transformer by jeonsworld. The code can be found [here](https://github.com/jeonsworld/ViT-pytorch/tree/main)